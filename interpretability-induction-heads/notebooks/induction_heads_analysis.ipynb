{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Induction Head Analysis Notebook\n",
    "\n",
    "This notebook provides a step-by-step walkthrough of induction head detection and analysis in GPT-2.\n",
    "\n",
    "## What are Induction Heads?\n",
    "\n",
    "Induction heads are attention heads that implement a simple but powerful pattern-completion algorithm:\n",
    "- When they see a token that appeared earlier in the sequence\n",
    "- They attend back to that earlier occurrence\n",
    "- This allows them to \"copy\" what came after the first occurrence\n",
    "\n",
    "For example, in the sequence `A B C D A`, an induction head would:\n",
    "1. At position 5 (second `A`), attend back to position 1 (first `A`)\n",
    "2. Enable the model to predict `B` as the next token (copying the pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from src.model import load_model, get_model_info, get_token_strs\n",
    "from src.data_gen import (\n",
    "    generate_repeated_sequence,\n",
    "    generate_corrupted_pair,\n",
    "    generate_batch,\n",
    "    get_token_set,\n",
    ")\n",
    "from src.analysis import (\n",
    "    analyze_induction_heads,\n",
    "    get_top_induction_heads,\n",
    "    get_attention_pattern,\n",
    ")\n",
    "from src.patching import run_patching_experiment\n",
    "from src.viz import (\n",
    "    plot_attention_pattern,\n",
    "    plot_induction_scores,\n",
    "    plot_induction_heatmap,\n",
    "    plot_patching_result,\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Model\n",
    "\n",
    "We use TransformerLens to load GPT-2 small with hooks for accessing internal activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 small\n",
    "model = load_model(\"gpt2-small\", device=\"cpu\")\n",
    "info = get_model_info(model)\n",
    "print(f\"Model: {info['name']}\")\n",
    "print(f\"Layers: {info['n_layers']}, Heads per layer: {info['n_heads']}\")\n",
    "print(f\"Model dimension: {info['d_model']}, Head dimension: {info['d_head']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Induction Prompts\n",
    "\n",
    "We create prompts with repeated patterns that trigger induction behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a single induction prompt\n",
    "vocab = get_token_set(\"letters\")\n",
    "prompt = generate_repeated_sequence(vocab, prefix_length=5, seed=42)\n",
    "\n",
    "print(f\"Prompt: {prompt.text}\")\n",
    "print(f\"Tokens: {prompt.tokens}\")\n",
    "print(f\"First occurrence position: {prompt.first_occurrence_pos}\")\n",
    "print(f\"Second occurrence position: {prompt.second_occurrence_pos}\")\n",
    "print(f\"Expected next token: {prompt.expected_next}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the model actually predicts\n",
    "tokens = model.to_tokens(prompt.text, prepend_bos=True)\n",
    "token_strs = get_token_strs(model, prompt.text)\n",
    "\n",
    "print(\"Tokenized sequence:\")\n",
    "for i, t in enumerate(token_strs):\n",
    "    print(f\"  Position {i}: '{t}'\")\n",
    "\n",
    "# Get model prediction\n",
    "with torch.no_grad():\n",
    "    logits = model(tokens)\n",
    "    probs = torch.softmax(logits[0, -1, :], dim=-1)\n",
    "    top_probs, top_indices = torch.topk(probs, 5)\n",
    "\n",
    "print(\"\\nTop 5 predictions for next token:\")\n",
    "for prob, idx in zip(top_probs, top_indices):\n",
    "    token = model.tokenizer.decode(idx.item())\n",
    "    print(f\"  '{token}': {prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Induction Heads\n",
    "\n",
    "We compute induction scores for all attention heads across multiple prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top induction heads by averaging over multiple prompts\n",
    "top_heads = get_top_induction_heads(\n",
    "    model,\n",
    "    vocab_name=\"letters\",\n",
    "    n_prompts=10,\n",
    "    prefix_length=5,\n",
    "    top_k=20,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Top 10 Induction Heads:\")\n",
    "print(\"-\" * 30)\n",
    "for i, (layer, head, score) in enumerate(top_heads[:10]):\n",
    "    print(f\"{i+1}. Layer {layer}, Head {head}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize induction scores\n",
    "from src.analysis import HeadScore\n",
    "\n",
    "head_scores = [\n",
    "    HeadScore(layer=l, head=h, score=s, attention_to_prev=0)\n",
    "    for l, h, s in top_heads\n",
    "]\n",
    "\n",
    "fig = plot_induction_scores(head_scores, top_k=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap view\n",
    "fig = plot_induction_heatmap(\n",
    "    head_scores,\n",
    "    n_layers=info['n_layers'],\n",
    "    n_heads=info['n_heads'],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Attention Patterns\n",
    "\n",
    "Let's look at the attention patterns of the top induction heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top induction head\n",
    "top_layer, top_head, top_score = top_heads[0]\n",
    "print(f\"Analyzing top induction head: Layer {top_layer}, Head {top_head} (score: {top_score:.4f})\")\n",
    "\n",
    "# Get attention pattern\n",
    "attn_pattern, tokens = get_attention_pattern(model, prompt.text, top_layer, top_head)\n",
    "\n",
    "# Plot\n",
    "fig = plot_attention_pattern(\n",
    "    attn_pattern,\n",
    "    tokens,\n",
    "    top_layer,\n",
    "    top_head,\n",
    "    highlight_positions=(prompt.second_occurrence_pos, prompt.first_occurrence_pos),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with a non-induction head (low score)\n",
    "low_layer, low_head, low_score = top_heads[-1]\n",
    "print(f\"Analyzing low-scoring head: Layer {low_layer}, Head {low_head} (score: {low_score:.4f})\")\n",
    "\n",
    "attn_pattern_low, _ = get_attention_pattern(model, prompt.text, low_layer, low_head)\n",
    "\n",
    "fig = plot_attention_pattern(\n",
    "    attn_pattern_low,\n",
    "    tokens,\n",
    "    low_layer,\n",
    "    low_head,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Activation Patching Experiment\n",
    "\n",
    "We demonstrate causal importance by patching activations from a clean run into a corrupted run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate clean and corrupted prompts\n",
    "clean_prompt, corrupted_prompt = generate_corrupted_pair(vocab, prefix_length=5, seed=42)\n",
    "\n",
    "print(\"Clean prompt:\", clean_prompt.text)\n",
    "print(\"Corrupted prompt:\", corrupted_prompt.text)\n",
    "print(f\"\\nExpected next token: '{clean_prompt.expected_next}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run patching experiment on top induction head\n",
    "result = run_patching_experiment(\n",
    "    model,\n",
    "    clean_prompt,\n",
    "    corrupted_prompt,\n",
    "    patch_type=\"attention_head\",\n",
    "    layer=top_layer,\n",
    "    head=top_head,\n",
    ")\n",
    "\n",
    "print(f\"Patching Layer {top_layer}, Head {top_head}:\")\n",
    "print(f\"  Clean probability: {result.clean_prob:.4f}\")\n",
    "print(f\"  Corrupted probability: {result.corrupted_prob:.4f}\")\n",
    "print(f\"  Patched probability: {result.patched_prob:.4f}\")\n",
    "print(f\"  Recovery ratio: {result.recovery_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize patching result\n",
    "fig = plot_patching_result(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare patching different heads\n",
    "print(\"Patching results for top 5 heads:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "results = []\n",
    "for layer, head, score in top_heads[:5]:\n",
    "    result = run_patching_experiment(\n",
    "        model,\n",
    "        clean_prompt,\n",
    "        corrupted_prompt,\n",
    "        patch_type=\"attention_head\",\n",
    "        layer=layer,\n",
    "        head=head,\n",
    "    )\n",
    "    results.append((layer, head, score, result))\n",
    "    print(f\"L{layer}H{head} (score={score:.4f}): recovery={result.recovery_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual stream patching\n",
    "print(\"\\nResidual stream patching by layer:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for layer in range(0, info['n_layers'], 2):  # Every other layer\n",
    "    result = run_patching_experiment(\n",
    "        model,\n",
    "        clean_prompt,\n",
    "        corrupted_prompt,\n",
    "        patch_type=\"residual_stream\",\n",
    "        layer=layer,\n",
    "        head=0,  # Not used for residual stream\n",
    "    )\n",
    "    print(f\"Layer {layer}: recovery={result.recovery_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Induction heads are identifiable**: We can detect attention heads with high induction scores by measuring attention from repeated tokens to their first occurrence.\n",
    "\n",
    "2. **Characteristic attention pattern**: Induction heads show a distinctive pattern of attending \"back\" to earlier positions with matching tokens.\n",
    "\n",
    "3. **Causal importance**: Activation patching confirms that induction heads are causally important for pattern completion. Patching their activations from clean to corrupted runs restores the model's ability to predict the correct next token.\n",
    "\n",
    "### In GPT-2 small:\n",
    "- Induction heads typically appear in middle-to-later layers (layers 5-8)\n",
    "- Not all heads are equally important; a few heads do most of the induction work\n",
    "- The induction mechanism is robust across different token types and sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save figures for documentation\n",
    "import os\n",
    "os.makedirs('../figures', exist_ok=True)\n",
    "\n",
    "# Induction scores\n",
    "fig = plot_induction_scores(head_scores, top_k=15)\n",
    "fig.savefig('../figures/induction_scores.png', dpi=150, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Heatmap\n",
    "fig = plot_induction_heatmap(head_scores, info['n_layers'], info['n_heads'])\n",
    "fig.savefig('../figures/induction_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Attention pattern\n",
    "attn_pattern, tokens = get_attention_pattern(model, prompt.text, top_layer, top_head)\n",
    "fig = plot_attention_pattern(attn_pattern, tokens, top_layer, top_head)\n",
    "fig.savefig('../figures/attention_pattern.png', dpi=150, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"Figures saved to ../figures/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
